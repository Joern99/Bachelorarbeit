
@article{cotter_medulloblastoma_2022,
	title = {Medulloblastoma: {WHO} 2021 and Beyond},
	volume = {25},
	issn = {1615-5742},
	doi = {10.1177/10935266211018931},
	shorttitle = {Medulloblastoma},
	abstract = {In 2016, medulloblastoma classification was restructured to allow for incorporation of updated data about medulloblastoma biology, genomics, and clinical behavior. For the first time, medulloblastomas were classified according to molecular characteristics ("genetically defined" categories) as well as histologic characteristics ("histologically defined" categories). Current genetically-defined categories include {WNT}-activated, {SHH}-activated {TP}53 wildtype, {SHH}-activated {TP}53-mutant, and non-{WNT}/non-{SHH}. In this article, we review the most recent update to the classification of medulloblastomas, provide a practical approach to immunohistochemical and molecular testing for these tumors, and demonstrate how to use key molecular genetic findings to develop an integrated diagnosis.},
	pages = {23--33},
	number = {1},
	journaltitle = {Pediatric and Developmental Pathology: The Official Journal of the Society for Pediatric Pathology and the Paediatric Pathology Society},
	shortjournal = {Pediatr Dev Pathol},
	author = {Cotter, Jennifer A. and Hawkins, Cynthia},
	date = {2022},
	pmid = {35168417},
	keywords = {Cerebellar Neoplasms, Humans, immunohistochemistry, Medulloblastoma, molecular pathology, neuropathology, oncology, Surg Path, tumors, World Health Organization},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Å ukasz and Polosukhin, Illia},
	urldate = {2025-10-12},
	date = {2017},
	file = {Full Text PDF:/Users/joern.ahlert/Zotero/storage/HNDWP5W9/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}
