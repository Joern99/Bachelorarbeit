Gezieltes Rater-Training mit Kalibrierungsbeispielen sollte implementiert werden, wie es in Abschnitt~\ref{subsubsec:analyse_menschliche_messung} als zentrales Defizit identifiziert wurde. Teilnehmende sollten vor der eigentlichen Bewertung gemeinsam Beispielfälle diskutieren und ihre Interpretationen der Bewertungsdimensionen abgleichen. Dies würde ein geteiltes Verständnis etablieren, ohne legitime subjektive Variabilität zu eliminieren. Ein strukturiertes Training könnte klären, ob die beobachteten niedrigen Konkordanzwerte primär auf unklare Instruktionen oder auf genuine konstruktbedingte Subjektivität zurückzuführen sind.

Die Stichprobengröße sollte deutlich erweitert werden. Wie in Abschnitt~\ref{subsec:Kendall W} dargelegt, gehen kleine Stichproben mit breiten Konfidenzintervallen für Kendall \(W\) einher, was die Präzision der Schätzungen reduziert. Eine Erweiterung auf mindestens 30–50 Rater bei 50–100 Frage-Antwort-Paaren würde robustere Reliabilitätsschätzungen ermöglichen und Subgruppenanalysen erlauben (vgl. Abschnitt~\ref{subsubsec:analyse_menschliche_messung}). Dies würde zudem die statistische Power erhöhen und klären, ob die beobachteten \(W\)-Werte stabil sind oder Artefakte der kleinen Stichprobe darstellen.

\subsubsection{Optimierung der Itemformulierungen und Erhebungsformate}

Die in Abschnitt~\ref{subsec:fragebogen-erhebung} verwendeten Itemformulierungen sollten überarbeitet werden. Begriffe wie „relevant", „logisch konsistent" oder „angemessen" wurden nicht durch konkrete Beispiele operationalisiert, sodass jeder Rater eigene Interpretationen entwickeln konnte. Die Bewertungsanweisungen müssen präziser spezifizieren, welche konkreten Aspekte zu bewerten sind. Mehrstufige Rubrics mit klaren Ankerbeispielen für jede Skalenstufe könnten die Konsistenz erhöhen.

Bereits jetzt erfolgt die Bewertung entlang theoretisch fundierter Dimensionen (vgl. Abschnitt~\ref{subsec:operationalisierung-plausibilitaet}). Um die Qualität und Verständlichkeit der Items weiter zu stärken, sollten gezielte Umfragen und Pretests mit potenziellen Ratern durchgeführt werden. Das Feedback kann genutzt werden, um Missverständnisse zu identifizieren und die Formulierungen iterativ zu verbessern, sodass die Dimensionen nicht nur theoretisch fundiert, sondern auch praktisch verständlich sind.

Ergänzend sollten alternative Erhebungsformate getestet werden. Paarweise Forced-Choice-Vergleiche könnten die Reliabilität steigern, da sie nachweislich konsistentere Urteile liefern als absolute Ratings auf Likert-Skalen (vgl. Abschnitt~\ref{subsubsec:gegenueberstellung_messmethoden}). Dies würde testen, ob die niedrigen Konkordanzwerte format-spezifisch sind oder auf fundamentale konzeptuelle Probleme bei der Operationalisierung von Plausibilität hindeuten.

\subsubsection{Theoriegeleitete Granularisierung der Bewertungsdimensionen}

Die in Abschnitt~\ref{subsubsec:gegenueberstellung_messmethoden} dokumentierten dimensionsspezifischen Korrelationen zwischen \(\tau = -0.214\) (Kohärenz) und \(\tau = 0.509\) (Angemessenheit) zeigen, dass die drei Operationalisierungsdimensionen unterschiedlich erfassbar sind. Die negative Korrelation bei Kohärenz deutet darauf hin, dass diese Dimension möglicherweise zu breit definiert ist. Eine Unterteilung in lokale Kohärenz (Satzübergänge, pronominale Referenzen) und globale Kohärenz (logische Widerspruchsfreiheit über den gesamten Text) könnte präzisere Bewertungen ermöglichen und klären, auf welcher Ebene menschliche und automatische Urteile divergieren.

Analog könnte Angemessenheit in formale Kriterien (Textlänge, Detailtiefe) und konzeptionelle Passung (thematische Relevanz, Adressatenbezug) getrennt werden. Dies würde isolieren, ob die moderate Übereinstimmung (\(\tau = 0.509\)) primär auf leicht erfassbare Oberflächenmerkmale zurückzuführen ist oder tatsächlich semantische Angemessenheit bewertet wird.

Das Plausibility Analysis Model nach \textcite{connell_model_2006} (Abschnitt~\ref{subsec:pam}) bietet mit den Parametern \textit{Corroboration}, \textit{Complexity} und \textit{Conjecture} einen theoretisch fundierten Rahmen, der im Experiment nur indirekt operationalisiert wurde. Eine direkte Implementierung dieser PAM-Parameter würde Rater instruieren, explizit zu bewerten, wie stark die Antwort durch den Kontext gestützt wird, wie komplex die kausale Erklärung ist und wie viele spekulative Zusatzannahmen erforderlich sind. Diese granulare Erfassung könnte klären, ob die beobachteten Divergenzen zwischen \ac{llm}s und Menschen systematisch bei bestimmten PAM-Parametern auftreten (vgl. Abschnitt~\ref{subsubsec:gegenueberstellung_messmethoden}).

Schließlich sollte die menschliche Bewertung stärker theoriegeleitet operationalisiert werden